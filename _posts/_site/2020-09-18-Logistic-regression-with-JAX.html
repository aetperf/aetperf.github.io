<p><a href="https://github.com/google/jax">JAX</a> is a Python package for <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> from Google Research. It is a really powerful and efficient library. JAX can automatically differentiate some Python code (supports the reverse- and forward-mode). It can also speed up the exection time by using the <a href="https://www.tensorflow.org/xla?hl=fi">XLA (Accelerated Linear Algebra)</a> compiler. JAX allows your code to run efficiently on CPUs, GPUs and TPUs. It is a library mainly used for machine learning. We refer to the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a> [2] for a very good introduction to JAX.</p>

<p align="center">
  <img width="600" src="/img/2020-09-18_01/JAX_village.jpg" alt="world" />
</p>

<p>Photo credit: <a href="http://www.cpauvergne.com/2018/08/jax.html">Papou Moustache</a></p>

<p>In this post we are going to simply use JAX’ <code class="language-plaintext highlighter-rouge">grad</code> function (back-propagation) to minimize the cost function of Logistic regression. In case you don’t know, <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic regression</a> is a supervised learning algorithm, for classification.</p>

<p>Here are the steps of this post:</p>
<ul>
  <li>load a toy dataset</li>
  <li>briefly describe Logistic regression</li>
  <li>derive the formulae for the Logistic regression cost</li>
  <li>create a cost gradient function with JAX</li>
  <li>learn the Logistic regression weights with two gradient-based minimization methods: Gradient descent and BFGS</li>
</ul>

<h2 id="imports">Imports</h2>

<p>We import JAX’ NumPy instead of the regular one.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">warnings</span>

<span class="n">warnings</span><span class="p">.</span><span class="nf">filterwarnings</span><span class="p">(</span><span class="sh">"</span><span class="s">ignore</span><span class="sh">"</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="nb">UserWarning</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="n">FS</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># figure size
</span><span class="n">RS</span> <span class="o">=</span> <span class="mi">124</span>  <span class="c1"># random seed
</span></code></pre></div></div>

<h2 id="load-split-and-scale-the-dataset">Load, split and scale the dataset</h2>

<p>The breast cancer dataset is a classic binary classification dataset that we load from <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">scikit-learn</a>. Dataset features:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Classes</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: right">Samples per class</td>
      <td style="text-align: right">212(0),357(1)</td>
    </tr>
    <tr>
      <td style="text-align: right">Samples total</td>
      <td style="text-align: right">569</td>
    </tr>
    <tr>
      <td style="text-align: right">Dimensionality</td>
      <td style="text-align: right">30</td>
    </tr>
    <tr>
      <td style="text-align: right">Features</td>
      <td style="text-align: right">real, positive</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">n_feat</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">RS</span>
<span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="logistic-regression">Logistic regression</h2>

<p>Here we are going to look at the binary classification case, but it is straightforward to generalize the algorithm to multiclass classification using One-vs-Rest, or multinomial (Softmax) logistic regression.</p>

<p>Assume that we have $k$ predictors:</p>

\[\left\{ X_i \right\}_{i=1}^{k} \in \mathbf{R}^k\]

<p>and a binary response variable:</p>

\[Y \in \left\{ 0, 1 \right\}\]

<p>In the logistic regression algorithm, the relationship between the predictors and the $logit$ of the probability of a positive outcome $Y=1$ is assumed to be linear:</p>

<p>\begin{equation}
logit( P(Y=1 | \textbf{w} ) ) = c +  \sum_{i=1}^k w_i X_i \tag{1}
\end{equation}</p>

<p>where</p>

\[\left\{ w_i \right\}_{i=1}^{k} \in \mathbf{R}^k\]

<p>are the linear weights and $c \in \mathbf{R}$ the intercept. Now what is the $logit$ function? It is the log of odds:</p>

<p>\begin{equation}
logit( p ) = \ln \left( \frac{p}{1-p} \right) \tag{2}
\end{equation}</p>

<p>We see that the $logit$ function is a way to map a probability value from $(0, 1)$ to $\mathbf{R}$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FS</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">p</span><span class="sh">"</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">$logit(p)$</span><span class="sh">"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">The $logit$ function</span><span class="sh">"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p align="center">
  <img width="600" src="/img/2020-09-18_01/output_7_0.png" alt="output_7_0" />
</p>

<p>The inverse of the $logit$ is the $logistic$ curve (also called sigmoid function), which we are going to note $\sigma$:</p>

<p>\begin{equation}
\sigma (r) = \frac{1}{1 + e^{-r}} \tag{3}
\end{equation}</p>

<p>Here is the implementation of the $logistic$ curve:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="p">))</span>


<span class="n">b</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FS</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">$logistic(r)$</span><span class="sh">"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">The $logistic$ curve</span><span class="sh">"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<p align="center">
  <img width="600" src="/img/2020-09-18_01/output_9_0.png" alt="output_9_0" />
</p>

<p>If we denote by $\textbf{w} = \left[c \; w_1 \; … \; w_k \right]^T$ the weight vector, $\textbf{x} = \left[ 1 \; x_1 \; … \;x_k \right]^T$ the observed values of the predictors, and $y$ the associated class value, we have:</p>

<p>\begin{equation}
logit( P(y=1 | \textbf{w} ) ) = \textbf{w}^T \textbf{x} \tag{4}
\end{equation}</p>

<p>And thus:</p>

<p>\begin{equation}
P(y=1 | \textbf{w} )=  \sigma(\textbf{w}^T \textbf{x} ) \equiv \sigma_{\textbf{w}} (\textbf{x}) \tag{5}
\end{equation}</p>

<p>For a given set of weights $\textbf{w}$, the probability of a positive outcome is $\sigma_{\textbf{w}} (\textbf{x})$ that we implement in the following <code class="language-plaintext highlighter-rouge">predict</code> function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span>
</code></pre></div></div>

<p>This probability can be turned into a predicted class label $\hat{y}$ using a threshold value:</p>

\[\hat{y} = 1 \;  \text{if} \; \sigma_{\textbf{w}} (\textbf{x}) \geq 0.5, \; 0 \; \text{otherwise} \tag{6}\]

<h2 id="the-cost-funtion">The cost funtion</h2>

<p>Now we assume that we have $n$ observations and that they are independently Bernoulli distributed:</p>

\[\left\{ \left( \textbf{x}^{(1)}, y^{(1)} \right), \left( \textbf{x}^{(2)}, y^{(2)} \right), ...,  \left( \textbf{x}^{(n)}, y^{(n)} \right) \right\}\]

<p>The likelihood that we would like to maximize given the samples is the following one:</p>

<p>\begin{equation}
L(\textbf{w}) = \prod_{i=1}^n P( y^{(i)} | \textbf{x}^{(i)}; \textbf{w}) = \prod_{i=1}^n \sigma_{\textbf{w}} \left(\textbf{x}^{(i)} \right)^{y^{(i)}}  \left( 1- \sigma_{\textbf{w}} \left(\textbf{x}^{(i)} \right)\right)^{1-y^{(i)}}   \tag{7}
\end{equation}</p>

<p>For some reasons related to numerical stability, we prefer to deal with a scaled log-likelihood. Also, we take the negative, in order to get a minimization problem:</p>

<p>\begin{equation}
J(\textbf{w}) = - \frac{1}{n} \sum_{i=1}^n \left[  y^{(i)} \log \left(  \sigma_{\textbf{w}} \left(\textbf{x}^{(i)} \right) \right) +  \left( 1-y^{(i)} \right) \log \left( 1- \sigma_{\textbf{w}} \left(\textbf{x}^{(i)} \right)\right) \right] \tag{8}
\end{equation}</p>

<p>A great feature of this cost function is that it is differentiable and convex. A gradient-based algorithm should find the global minimum. Now let’s also introduce some $l_2$-regularization in order to improve the model:</p>

<p>\begin{equation}
J_r(\textbf{w}) = J(\textbf{w})  + \frac{\lambda}{2} \textbf{w}^T \textbf{w} \tag{9}
\end{equation}</p>

<p>with $\lambda \geq 0$. As written by Sebastian Raschka in [1]:</p>

<blockquote>
  <p>Regularization is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting.</p>
</blockquote>

<p>Here is the cost function from Eq.$(9)$, $J_r(\textbf{w})$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-14</span><span class="p">,</span> <span class="n">lmbd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># bound the probabilities within (0,1) to avoid ln(0)
</span>    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">lmbd</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">c</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>We can now evaluate the cost function for some given values of $\textbf{w}$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c_0</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">w_0</span> <span class="o">=</span> <span class="mf">1.0e-5</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_feat</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">cost</span><span class="p">(</span><span class="n">c_0</span><span class="p">,</span> <span class="n">w_0</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7271773
</code></pre></div></div>

<p>We can also perform a prediction on the test dataset, but using weights that are very far from optimal:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_proba</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c_0</span><span class="p">,</span> <span class="n">w_0</span><span class="p">,</span> <span class="n">X_test_s</span><span class="p">)</span>
<span class="n">y_pred_proba</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DeviceArray([0.7310729 , 0.7310529 , 0.73104334, 0.7310562 , 0.7310334 ],            dtype=float32)
</code></pre></div></div>

<p>and convert the resulting probabilities to predicted class labels:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DeviceArray([1., 1., 1., 1., 1.], dtype=float32)
</code></pre></div></div>

<p>This prediction is not so good, as expected:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support

           0       0.00      0.00      0.00        57
           1       0.60      1.00      0.75        86

    accuracy                           0.60       143
   macro avg       0.30      0.50      0.38       143
weighted avg       0.36      0.60      0.45       143
</code></pre></div></div>

<h2 id="learning-the-weights">Learning the weights</h2>

<p>So we need to minimize $J_r(\textbf{w})$. For that we are going to apply two different algorithms:</p>
<ul>
  <li>Gradient descent</li>
  <li>BFGS</li>
</ul>

<p>They both use gradient $\nabla_{\textbf{w}} J_r(\textbf{w})$.</p>

<h3 id="compute-the-gradient">Compute the gradient</h3>

<p>We could definitely compute the gradient of this Logistic regression cost function analytically. However we won’t, because we are are lazy and want JAX to do it for us! However, we can say that JAX would be more relevant if applied to a very complex function for which an analytical derivative is very hard or impossible to compute, such as the cost function of a deep neural network for example.</p>

<p>So let’s differentiate this <code class="language-plaintext highlighter-rouge">cost</code> function with respect to the first and second positional arguments using JAX’ <code class="language-plaintext highlighter-rouge">grad</code> function. Here is the derivative with respect to the intercept $c$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">c_0</span><span class="p">,</span> <span class="n">w_0</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.19490835
</code></pre></div></div>

<p>And here is the gradient with respect to the other weights $\left[w_1 \; … \; w_k \right]^T$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">c_0</span><span class="p">,</span> <span class="n">w_0</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 0.3548751   0.19858086  0.36013606  0.3432787   0.16739811  0.28374672
  0.3358652   0.37103614  0.16127907 -0.01301905  0.2716888  -0.02297289
  0.26268682  0.25861    -0.05540825  0.14209975  0.12593843  0.19165947
 -0.02546574  0.03931254  0.37642777  0.21218807  0.37781695  0.3545265
  0.18594304  0.28257003  0.31803223  0.37415543  0.19396219  0.15303871]
</code></pre></div></div>

<p>Note that the <code class="language-plaintext highlighter-rouge">grad</code> function returns a function.</p>

<h3 id="gradient-descent">Gradient descent</h3>

<p>From <a href="https://en.wikipedia.org/wiki/Gradient_descent">wikipedia</a>:</p>
<blockquote>
  <p>Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.</p>
</blockquote>

<p>The Gradient descent algorithm is very basic, here is an outline:</p>

<p>$w=w_0$   <br />
for $i = 1, …, n_{iter}$:<br />
$ \hspace{1cm} w \leftarrow w - \eta \nabla_{\textbf{w}} J_r(\textbf{w})$<br />
with $\eta &gt;0$ small enough (that we can see as the learning rate).</p>

<p>And here is an implementation in which we added a stopping criterion (exits the loop if it stagnates during 20 iterations):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">5e-2</span>
<span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w_0</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">c_0</span>
<span class="n">new_cost</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="nf">cost</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="n">cost_hist</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_cost</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="n">c_current</span> <span class="o">=</span> <span class="n">c</span>
    <span class="n">c</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="nf">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">c_current</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="nf">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">c_current</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">new_cost</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="nf">cost</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">cost_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_cost</span><span class="p">)</span>
    <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">cost_hist</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">cost_hist</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Exited loop at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Exited loop at iteration 680
CPU times: user 20.6 s, sys: 1.8 s, total: 22.4 s
Wall time: 18.5 s
</code></pre></div></div>

<p>Let’s plot the convergence history:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FS</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">semilogy</span><span class="p">(</span><span class="n">cost_hist</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Iteration</span><span class="sh">"</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Cost value</span><span class="sh">"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Convergence history</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p align="center">
  <img width="600" src="/img/2020-09-18_01/output_33_0.png" alt="output_33_0" />
</p>

<p>We can evaluate the trained model on the test set and check that the result is OK:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_proba</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_test_s</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support

           0       0.96      0.96      0.96        57
           1       0.98      0.98      0.98        86

    accuracy                           0.97       143
   macro avg       0.97      0.97      0.97       143
weighted avg       0.97      0.97      0.97       143
</code></pre></div></div>

<h3 id="bfgs">BFGS</h3>

<p>From <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">wikipedia</a>:</p>
<blockquote>
  <p>The BFGS method belongs to quasi-Newton methods, a class of hill-climbing optimization techniques that seek a stationary point of a (preferably twice continuously differentiable) function.</p>
</blockquote>

<p>We are going to use <a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs">SciPy’s implementation</a> and give the <code class="language-plaintext highlighter-rouge">grad</code> function from JAX as an input parameter. Let’s first define the objective function with a single input vector (instead of <code class="language-plaintext highlighter-rouge">c</code> and <code class="language-plaintext highlighter-rouge">w</code>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">coefs</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="nf">cost</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">res</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">,</span>
    <span class="n">jnp</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">c_0</span><span class="p">,</span> <span class="n">w_0</span><span class="p">]),</span>
    <span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">BFGS</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">jac</span><span class="o">=</span><span class="nf">grad</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">gtol</span><span class="sh">"</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="sh">"</span><span class="s">disp</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Optimization terminated successfully.
         Current function value: 0.209017
         Iterations: 15
         Function evaluations: 16
         Gradient evaluations: 16
CPU times: user 480 ms, sys: 39.9 ms, total: 520 ms
Wall time: 445 ms
</code></pre></div></div>

<p>Much faster with a similar result!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_test_s</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support

           0       0.96      0.96      0.96        57
           1       0.98      0.98      0.98        86

    accuracy                           0.97       143
   macro avg       0.97      0.97      0.97       143
weighted avg       0.97      0.97      0.97       143
</code></pre></div></div>

<p>Well we hardly scratched the surface of what can be done with JAX, but at least we presented a little example.</p>

<h2 id="references">References:</h2>

<p>Both references are really great ressources:</p>

<p>[1] S. Raschka and V. Mirjalili, <em>Python Machine Learning, 2nd edition</em>, Packt Publishing Ltd, Packt Publishing Ltd, 2017.</p>

<p>[2] alexbw@, mattjj@, <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"><em>The Autodiff Cookbook</em></a></p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://aetperf-github-io-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

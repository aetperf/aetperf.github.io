<p align="center">
  <img width="200" src="/img/2021-01-29_01/1024px-Keras_logo.svg.png" alt="Keras logo" />
</p>

<p>Training a DL model might take some time, and make use of some special hardware. So you may want to save the model for using it later, or on another computer.</p>

<p>In this short Python notebook, we are going to create a very simple <code class="language-plaintext highlighter-rouge">tensorflow.keras</code> model, train it, save it into a directory along with the training data scaling factors (standard scaling), and then load and call it.</p>

<p>The dataset is the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">breast cancer dataset</a> from scikit-learn (binary classification). The point of this post is not the model but rather saving and loading the entire Keras model with the training dataset feature-wise mean and std.</p>

<h2 id="imports">Imports</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">RS</span> <span class="o">=</span> <span class="mi">124</span>  <span class="c1"># random state
</span></code></pre></div></div>

<p>The package versions are the following ones :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CPython    3.7.8
tensorflow 2.3.0
pandas     0.25.3
sklearn    0.23.2
numpy      1.18.5
</code></pre></div></div>

<h2 id="data-loading-and-train-test-split">Data loading and train test split</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">RS</span>
<span class="p">)</span>
<span class="n">X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(569, 30)
</code></pre></div></div>

<h2 id="data-standardization-with-a-normalization-layer">Data standardization with a Normalization layer</h2>

<p>We are going to use a <code class="language-plaintext highlighter-rouge">Normalization layer</code>. As explained in the <a href="https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/normalization/">documentation</a> :</p>

<blockquote>
  <p>This layer will coerce its inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling (input-mean)/sqrt(var) at runtime.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="nc">Normalization</span><span class="p">()</span>
<span class="n">layer</span><span class="p">.</span><span class="nf">adapt</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="keras-model">Keras model</h2>

<p>We use a very simple model :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">set_seed</span><span class="p">(</span><span class="n">RS</span><span class="p">)</span>  <span class="c1"># Sets the random seed
</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">layer</span><span class="p">,</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span>
            <span class="mi">30</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_size</span><span class="p">,),</span>
        <span class="p">),</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span>
            <span class="mi">16</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">sigmoid</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">binary_crossentropy</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
normalization_1 (Normalizati (None, 30)                61        
_________________________________________________________________
dense_3 (Dense)              (None, 30)                930       
_________________________________________________________________
dense_4 (Dense)              (None, 16)                496       
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 17        
=================================================================
Total params: 1,504
Trainable params: 1,443
Non-trainable params: 61
_________________________________________________________________
None
</code></pre></div></div>

<h2 id="training">Training</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ax</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">).</span><span class="nf">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Training loss and accuracy</span><span class="sh">"</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Epochs</span><span class="sh">"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">"</span><span class="s">Training loss</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Trainig accuracy</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p align="center">
  <img width="600" src="/img/2021-01-29_01/output_14_0.png" alt="Training loss and accuracy" />
</p>

<p>If we evaluate the model on the test set, we get an accuracy of 0.9708:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6/6 [==============================] - 0s 12ms/step - loss: 0.1053 - accuracy: 0.9708
</code></pre></div></div>

<p>OK, now that the model is trained, let’s save it!</p>

<h2 id="save-the-whole-model">Save the whole model</h2>

<p>It is possible to partially save the model. However, here we are going to save the entire model with <code class="language-plaintext highlighter-rouge">model.save()</code>. As described in the <a href="https://www.tensorflow.org/guide/keras/save_and_serialize">documentation</a> :</p>

<blockquote>
  <p>You can save an entire model to a single artifact. It will include :<br />
    - The model’s architecture/config<br />
    - The model’s weight values (which were learned during training)<br />
    - The model’s compilation information (if compile()) was called<br />
    - The optimizer and its state, if any (this enables you to restart training where you left)</p>
</blockquote>

<p>Note that since we only load the model for inference in the later part of the post, we do not actually need the two last points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./tf_model</span><span class="sh">"</span>
<span class="n">model</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Assets written to: ./tf_model/assets
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">tree</span> <span class="p">{</span><span class="n">MODEL_PATH</span><span class="p">}</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./tf_model
├── assets
├── saved_model.pb
└── variables
    ├── variables.data-00000-of-00001
    └── variables.index

2 directories, 3 files
</code></pre></div></div>

<blockquote>
  <p>The model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in <code class="language-plaintext highlighter-rouge">saved_model.pb</code>. The weights are saved in the <code class="language-plaintext highlighter-rouge">variables/</code> directory.</p>
</blockquote>

<h2 id="loading-the-model">Loading the model</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="calling-the-model">Calling the model</h2>

<p>Note that the normalization layer is automatically used in the inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_pred_proba</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.02198085, 0.37227017, 0.0198662 , 0.99156594], dtype=float32)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">y_pred_proba</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0, 0, 0, 1])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">accuracy score : </span><span class="si">{</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="mf">5.4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accuracy score : 0.9708
</code></pre></div></div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://aetperf-github-io-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

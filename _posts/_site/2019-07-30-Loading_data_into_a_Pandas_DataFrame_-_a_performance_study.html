<p><img src="/img/2019-07-30_01/moebius01.jpg" alt="moebius" title="Moebius" /></p>

<p>Because doing machine learning implies trying many options and algorithms with different parameters, from data cleaning to model validation, the <strong>Python</strong> programmers will often load a full dataset into a <a href="https://pandas.pydata.org/">Pandas</a> dataframe, without actually modifying the stored data. This loading part might seem relatively long sometimes… In this post, we look at different options regarding the storage, in terms of elapsed time and disk space.</p>

<p>We are going to measure the <strong>loading time</strong> of a small- to medium-size table stored in different formats, either in a file (CSV file, <a href="https://github.com/wesm/feather">Feather</a>, <a href="https://parquet.apache.org/">Parquet</a> or 
 <a href="https://support.hdfgroup.org/HDF5/whatishdf5.html">HDF5</a>) or in a database (Microsoft SQL Server). For the file storage formats (as opposed to DB storage, even if DB stores data in files…), we also look at <strong>file size</strong> on disk.</p>

<p>Measurements are going to be performed for different tables lengths, table widths and “data entropy” (number of unique values per columns).</p>

<p>This performance study is inspired by this great post <a href="http://wesmckinney.com/blog/python-parquet-multithreading/"><em>Extreme IO performance with parallel Apache Parquet in Python</em></a> by <a href="https://wesmckinney.com/">Wes McKinney</a>.</p>

<h2 id="introduction">Introduction</h2>

<p>Let’s start by giving the complete list of the storage formats evaluated in the present post, describing the hardware and software environment, as well as the fake data set.</p>

<h3 id="complete-list-of-storage-formats">Complete list of storage formats</h3>

<p>Here is the list of the different options we used for saving the data and the Pandas function used to load:</p>
<ul>
  <li><strong>MSSQL_pymssql</strong> :  Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html">read_sql()</a> with MS SQL and a <a href="https://github.com/pymssql/pymssql">pymssql</a> connection</li>
  <li><strong>MSSQL_pyodbc</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html">read_sql()</a> with MS SQL and a <a href="https://github.com/mkleehammer/pyodbc">pyodbc</a> connection</li>
  <li><strong>MSSQL_turbobdc</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html">read_sql()</a> with MS SQL and a <a href="https://turbodbc.readthedocs.io/en/latest/">turbobdc</a> connection</li>
  <li><strong>CSV</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv">read_csv()</a> for comma-separated values files</li>
  <li><strong>Parquet_fastparquet</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html#pandas.read_parquet">read_parquet()</a> with the <a href="https://github.com/dask/fastparquet">fastparquet</a> engine. File saved without compression</li>
  <li><strong>Parquet_fastparquet_gzip</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html#pandas.read_parquet">read_parquet()</a> with the <a href="https://github.com/dask/fastparquet">fastparquet</a> engine. File saved with <em>gzip</em> compression</li>
  <li><strong>Parquet_pyarrow</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html#pandas.read_parquet">read_parquet()</a> with the <a href="https://arrow.apache.org/docs/python/">pyarrow</a> engine. File saved without compression</li>
  <li><strong>Parquet_pyarrow_gzip</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html#pandas.read_parquet">read_parquet()</a> with the <a href="https://arrow.apache.org/docs/python/">pyarrow</a> engine. File saved with <em>gzip</em> compression</li>
  <li><strong>Feather</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_feather.html#pandas.read_feather">read_feather()</a></li>
  <li><strong>HDF_table</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html#pandas.read_hdf">read_hdf()</a>. File saved with the <code class="language-plaintext highlighter-rouge">table</code> option. From Pandas’ documentation:
    <blockquote>
      <p>write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data</p>
    </blockquote>
  </li>
  <li><strong>HDF_fixed</strong> : Pandas’ <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html#pandas.read_hdf">read_hdf()</a>. File saved with the <code class="language-plaintext highlighter-rouge">fixed</code> option. From Pandas’ documentation:
    <blockquote>
      <p>fast writing/reading. Not-appendable, nor searchable</p>
    </blockquote>
  </li>
</ul>

<h3 id="data-creation">Data creation</h3>

<p>For the purpose of the comparison, we are going to create a fake table dataset of variable length <code class="language-plaintext highlighter-rouge">n</code>, and variable number of columns:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">n_int</code> columns of <code class="language-plaintext highlighter-rouge">int64</code> type, generated randomnly in the between 0 and <code class="language-plaintext highlighter-rouge">i_max</code>-1,</li>
  <li><code class="language-plaintext highlighter-rouge">n_float</code> columns of <code class="language-plaintext highlighter-rouge">float64</code> type, drawn randomly between 0 and 1,</li>
  <li><code class="language-plaintext highlighter-rouge">n_str</code> columns of <code class="language-plaintext highlighter-rouge">category</code> type, of categorigal data with <code class="language-plaintext highlighter-rouge">n_cat</code> unique <code class="language-plaintext highlighter-rouge">str</code> in each column (<code class="language-plaintext highlighter-rouge">n_cat</code> different words drawn randomly from Shakespeare’s <em>King Lear</em>)</li>
</ul>

<p>So we have a total of <code class="language-plaintext highlighter-rouge">n_col</code> = <code class="language-plaintext highlighter-rouge">n_int</code> + <code class="language-plaintext highlighter-rouge">n_float</code> + <code class="language-plaintext highlighter-rouge">n_str</code> columns. The parameters <code class="language-plaintext highlighter-rouge">i_max</code> and <code class="language-plaintext highlighter-rouge">n_cat</code> control the “level of entropy” in the integer and categorical columns, i.e. the number of unique values per column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="nf">create_table</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_int</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_float</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_str</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">i_max</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_cat</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   I00  I01  I02  I03  I04       F00       F01       F02       F03       F04  \
0   18   27   46    9   17  0.394370  0.731073  0.161069  0.600699  0.865864   
1    3   11   26   28   11  0.983522  0.079366  0.428347  0.204543  0.450636   

        S00   S01       S02        S03     S04  
0      Cure  deer  unsettle    weakens  wicked  
1  banished  wrap     Loyal  fortnight  wicked  
</code></pre></div></div>

<h3 id="hardware">Hardware</h3>

<p>All measurements are performed on the same laptop with these feats:</p>
<ul>
  <li>CPU: <em>Intel(R) Core(TM) i7-7700HQ (8 cores) @2.80GHz</em></li>
  <li>RAM: <em>DDR4-2400, 16GB</em></li>
  <li>Disk: <em>Samsung SSD 970 PRO NVMe M.2, 1TB (average read rate: 3,3 GB/s)</em></li>
</ul>

<h3 id="libraries">Libraries</h3>

<p>We use the Anaconda distibution of <code class="language-plaintext highlighter-rouge">CPython 3.7.3</code>and the following package versions:</p>

<table>
  <thead>
    <tr>
      <th>package</th>
      <th>version</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pandas</td>
      <td>0.24.2</td>
    </tr>
    <tr>
      <td>sqlalchemy</td>
      <td>1.3.5</td>
    </tr>
    <tr>
      <td>pymssql</td>
      <td>2.1.4</td>
    </tr>
    <tr>
      <td>pyodbc</td>
      <td>4.0.24</td>
    </tr>
    <tr>
      <td>turbodbc</td>
      <td>3.1.1</td>
    </tr>
    <tr>
      <td>fastparquet</td>
      <td>0.3.1</td>
    </tr>
    <tr>
      <td>pyarrow</td>
      <td>0.13.0</td>
    </tr>
    <tr>
      <td>tables</td>
      <td>3.5.2</td>
    </tr>
  </tbody>
</table>

<h3 id="time-measurements-and-equality-tests">Time measurements and equality tests</h3>

<p>When measuring the reading time, we always keep the best out of three successive measurements.</p>

<p>Note that only the <strong>HDF_table</strong> format support writing and reading columns of <code class="language-plaintext highlighter-rouge">category</code> type. So for all the other formats, categorical columns are converted back into <code class="language-plaintext highlighter-rouge">dtype=category</code> after being read (this operation is included in the reading time measurement). Actually, 
we also make sure that the list of the different categories is always in lexicographical order:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categories</span> <span class="o">=</span> <span class="nc">CategoricalDtype</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">unique</span><span class="p">()),</span> <span class="n">ordered</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>
</code></pre></div></div>

<p>For the <strong>HDF_fixed</strong> format, one further have to explicitly convert the categorical column into strings before writing the dataframe into a file.</p>

<p>Also, we checked that the read data is exactly the same as the written data by using a small dataframe (only a few rows), storing it in each format, reading it and comparing the input and output dataframes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_frame_equal</span><span class="p">(</span><span class="n">df_written</span><span class="p">,</span> <span class="n">df_read</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="results">Results</h2>

<p>First, we change the table length but keep a fixed number of columns, then vary the number of columns with a fixed length. Finally we are going to change the number of unique values in each <code class="language-plaintext highlighter-rouge">int</code> and <code class="language-plaintext highlighter-rouge">category</code> columns (for a fixed number of rows and columns).</p>

<h3 id="loop-on-different-lengths">Loop on different lengths</h3>

<p>We loop on different table lengths <code class="language-plaintext highlighter-rouge">n</code>, from 10 to 1000000, with the following set of parameter values: <code class="language-plaintext highlighter-rouge">n_int</code>=5, <code class="language-plaintext highlighter-rouge">n_float</code>=5, <code class="language-plaintext highlighter-rouge">n_str</code>=5, <code class="language-plaintext highlighter-rouge">i_max</code>=50, <code class="language-plaintext highlighter-rouge">n_cat</code>=10.</p>

<p><img src="/img/2019-07-30_01/output_32_0.png" alt="png" /></p>

<p>In the above figure, the format are sorted in ascending order of reading time for the largest table length (<code class="language-plaintext highlighter-rouge">n</code>=1000000). This is why the <strong>HDF_table</strong> format appears first. However we can observe that this format performs poorly on small table sizes. Overall, <strong>Parquet_pyarrow</strong> is the fastest reading format for the given tables. The <strong>Parquet_pyarrow</strong> format is about 3 times as fast as the <strong>CSV</strong> one.</p>

<p>Also, regarding the Microsoft SQL storage, it is interesting to see that <strong>turbobdc</strong> performs slightly better than the two other drivers (<strong>pyodbc</strong> and <strong>pymssql</strong>). It actually achieves similar timings as <strong>CSV</strong>, which is not bad.</p>

<p>Now let’s have a look at the file size of the stored tables. First, here is the memory usage of each dataframe:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> <code class="language-plaintext highlighter-rouge">n</code></th>
      <th style="text-align: right">memory usage in MB</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">0.005</td>
    </tr>
    <tr>
      <td style="text-align: right">100</td>
      <td style="text-align: right">0.014</td>
    </tr>
    <tr>
      <td style="text-align: right">1000</td>
      <td style="text-align: right">0.087</td>
    </tr>
    <tr>
      <td style="text-align: right">10000</td>
      <td style="text-align: right">0.817</td>
    </tr>
    <tr>
      <td style="text-align: right">100000</td>
      <td style="text-align: right">8.112</td>
    </tr>
    <tr>
      <td style="text-align: right">1000000</td>
      <td style="text-align: right">81.068</td>
    </tr>
  </tbody>
</table>

<p>We can see that the tables are pretty small: at most 81 MB! In the next figure, we measure the file size for each table size, excluding the 3 MS SQL formats:</p>

<p><img src="/img/2019-07-30_01/output_33_0.png" alt="png" /></p>

<p>It appears that the <strong>Parquet_fastparquet_gzip</strong>, <strong>Parquet_pyarrow_gzip</strong> and <strong>Parquet_pyarrow</strong> formats are rather compact. <strong>HDF</strong> formats seems rather inadequate when dealing with small tables. The <strong>Parquet_pyarrow_gzip</strong> file is about 3 times smaller than the <strong>CSV</strong> one.</p>

<p>Also, note that many of these formats use equal or more space to store the data on a file than in memory (<strong>Feather</strong>, <strong>Parquet_fastparquet</strong>, <strong>HDF_table</strong>, <strong>HDF_fixed</strong>, <strong>CSV</strong>). This might be because the categorical columns are stored as <code class="language-plaintext highlighter-rouge">str</code> columns in the files, which is a redundant storage.</p>

<p>Now let us focus on longer tables in order to see whether <strong>HDF</strong> or <strong>Parquet</strong> performs better. We do not consider a DB storage anymore (tests are too long to perform, especially for data injection), but only single file storage. We set <code class="language-plaintext highlighter-rouge">n</code>=10000000 (memory usage of the dataframe: 810.629 MB).</p>

<p><img src="/img/2019-07-30_01/output_32_1.png" alt="png" /></p>

<p>We observe above that <strong>HDF_table</strong> is faster than other file formats for longer tables, but this may be partly because we don’t have to convert the string columns back into the categorical type. At best, it only takes 3.77 s to load the data, instead of 29.80 s for <strong>CSV</strong> format (8.21 s for <strong>Parquet_pyarrow</strong>). On the other hand, file size is larger when using <strong>HDF_table</strong> (932.57 MB) than <strong>Parquet_pyarrow</strong> (464.13 MB). The <strong>CSV</strong> format file is is the largest: 1522.37 MB!</p>

<p><img src="/img/2019-07-30_01/output_34_1.png" alt="png" /></p>

<h2 id="loop-on-different-widths">Loop on different widths</h2>

<p>The table length <code class="language-plaintext highlighter-rouge">n</code> is now fixed (<code class="language-plaintext highlighter-rouge">n</code>=100000) and only the number of columns is changed, always with the same proportion of int, float and categorical columns (1/3 each). We start from 15 and increases the column count up to 150. Here are the results:</p>

<p><img src="/img/2019-07-30_01/output_36_0.png" alt="png" /></p>

<p>We can observe that <strong>Parquet_fastparquet</strong> behaves better with larger tables, as opposed to <strong>HDF_table</strong>. <strong>Parquet_pyarrow</strong> still is in the leading trio. Still, <strong>MSSQL_turbobdc</strong> outperforms the two other MSSQL drivers.</p>

<p>If we look at the file size, we note that <strong>HDF</strong> files are rather large as compared to <strong>Parquet_fastparquet_gzip</strong> or <strong>Parquet_pyarrow_gzip</strong>.</p>

<p><img src="/img/2019-07-30_01/output_37_0.png" alt="png" /></p>

<p>It seems that the gzip compression is really more effective when applied to <strong>Parquet_fastparquet</strong> than <strong>Parquet_pyarrow</strong>, for which it is almost useless.</p>

<h2 id="loop-on-different-entropies">Loop on different entropies</h2>

<p>For this section, we are just going to look at the elapsed time (not the file size). The table length <code class="language-plaintext highlighter-rouge">n</code> is still fixed (<code class="language-plaintext highlighter-rouge">n</code>=100000), as well as the number of columns: 100 (<code class="language-plaintext highlighter-rouge">n_int</code>=50, <code class="language-plaintext highlighter-rouge">n_float</code>=0, <code class="language-plaintext highlighter-rouge">n_str</code>=50). Only the “entropy coefficient” <code class="language-plaintext highlighter-rouge">e</code> does vary betwen 0 and 1000. <code class="language-plaintext highlighter-rouge">e</code> is related to the number of unique values in each column in the following way: <code class="language-plaintext highlighter-rouge">i_max</code> = 50 + <code class="language-plaintext highlighter-rouge">e</code> and <code class="language-plaintext highlighter-rouge">n_cat</code> = 10 + <code class="language-plaintext highlighter-rouge">e</code>.</p>

<p><img src="/img/2019-07-30_01/output_40_0.png" alt="png" /></p>

<p>From the above figure, it seems that the “entropy coef” has a very small influence on the loading time for most of the formats. Surprisingly, it does have some kind of influence on the reading time of CSV files. It also have a non-negligible influence on all the <strong>Parquet</strong> formats.</p>

<h2 id="conclusion">Conclusion</h2>

<p>It is important to stress that we are not dealing here with big data processing, but rather very common small to medium size datasets. Here is what we can get from this performance study:</p>

<ul>
  <li><strong>Parquet_pyarrow</strong> is a good choice in most cases regarding both loading time and disk space</li>
  <li><strong>HDF_table</strong> is the fastest format when dealing with larger datasets.</li>
  <li><strong>MSSQL_turbobdc</strong> is rather efficient as compared to other MSSQL drivers, achieving similar timings as the CSV file format</li>
</ul>

<p>Please drop a comment if ever you think something in this post is not clear or inaccurate!</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://aetperf-github-io-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

